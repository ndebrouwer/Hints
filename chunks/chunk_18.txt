        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function wtnsCalculate(input, wasmFileName, wtnsFileName, options) {

        const fdWasm = await readExisting(wasmFileName);
        const wasm = await fdWasm.read(fdWasm.totalSize);
        await fdWasm.close();

        const wc = await builder(wasm);
        if (wc.circom_version() == 1) {
            const w = await wc.calculateBinWitness(input);

            const fdWtns = await createBinFile(wtnsFileName, "wtns", 2, 2);

            await writeBin(fdWtns, w, wc.prime);
            await fdWtns.close();
        } else {
            const fdWtns = await createOverride(wtnsFileName);

            const w = await wc.calculateWTNSBin(input);

            await fdWtns.write(w);
            await fdWtns.close();
        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function groth16FullProve(input, wasmFile, zkeyFileName, logger) {
        const wtns= {
            type: "mem"
        };
        console.time("witness calculation");
        await wtnsCalculate(input, wasmFile, wtns);
        console.timeEnd("witness calculation");

        console.time("groth16 prove");
        const proof = await groth16Prove(zkeyFileName, wtns, logger);
        console.timeEnd("groth16 prove");
        return proof;
    }

    /*
        Copyright 2018 0kims association.

        This file is part of snarkjs.

        snarkjs is a free software: you can redistribute it and/or
        modify it under the terms of the GNU General Public License as published by the
        Free Software Foundation, either version 3 of the License, or (at your option)
        any later version.

        snarkjs is distributed in the hope that it will be useful,
        but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
        more details.

        You should have received a copy of the GNU General Public License along with
        snarkjs. If not, see <https://www.gnu.org/licenses/>.
    */
    const {unstringifyBigInts: unstringifyBigInts$1} = utils;

    async function groth16Verify(vk_verifier, publicSignals, proof, logger) {
    /*
        let cpub = vk_verifier.IC[0];
        for (let s= 0; s< vk_verifier.nPublic; s++) {
            cpub  = G1.add( cpub, G1.timesScalar( vk_verifier.IC[s+1], publicSignals[s]));
        }
    */

        vk_verifier = unstringifyBigInts$1(vk_verifier);
        proof = unstringifyBigInts$1(proof);
        publicSignals = unstringifyBigInts$1(publicSignals);

        const curve = await getCurveFromName(vk_verifier.curve);

        const IC0 = curve.G1.fromObject(vk_verifier.IC[0]);
        const IC = new Uint8Array(curve.G1.F.n8*2 * publicSignals.length);
        const w = new Uint8Array(curve.Fr.n8 * publicSignals.length);

        for (let i=0; i<publicSignals.length; i++) {
            const buffP = curve.G1.fromObject(vk_verifier.IC[i+1]);
            IC.set(buffP, i*curve.G1.F.n8*2);
            Scalar.toRprLE(w, curve.Fr.n8*i, publicSignals[i], curve.Fr.n8);
        }

        let cpub = await curve.G1.multiExpAffine(IC, w);
        cpub = curve.G1.add(cpub, IC0);

        const pi_a = curve.G1.fromObject(proof.pi_a);
        const pi_b = curve.G2.fromObject(proof.pi_b);
        const pi_c = curve.G1.fromObject(proof.pi_c);

        const vk_gamma_2 = curve.G2.fromObject(vk_verifier.vk_gamma_2);
        const vk_delta_2 = curve.G2.fromObject(vk_verifier.vk_delta_2);
        const vk_alpha_1 = curve.G1.fromObject(vk_verifier.vk_alpha_1);
        const vk_beta_2 = curve.G2.fromObject(vk_verifier.vk_beta_2);

        const res = await curve.pairingEq(
            curve.G1.neg(pi_a) , pi_b,
            cpub , vk_gamma_2,
            pi_c , vk_delta_2,

            vk_alpha_1, vk_beta_2
        );

        if (! res) {
            if (logger) logger.error("Invalid proof");
            return false;
        }

        if (logger) logger.info("OK!");
        return true;
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    function p256$1(n) {
        let nstr = n.toString(16);
        while (nstr.length < 64) nstr = "0"+nstr;
        nstr = `"0x${nstr}"`;
        return nstr;
    }

    async function groth16ExportSolidityCallData(proof, pub) {

        let inputs = "";
        for (let i=0; i<pub.length; i++) {
            if (inputs != "") inputs = inputs + ",";
            inputs = inputs + p256$1(pub[i]);
        }

        let S;
        S=`[${p256$1(proof.pi_a[0])}, ${p256$1(proof.pi_a[1])}],` +
            `[[${p256$1(proof.pi_b[0][1])}, ${p256$1(proof.pi_b[0][0])}],[${p256$1(proof.pi_b[1][1])}, ${p256$1(proof.pi_b[1][0])}]],` +
            `[${p256$1(proof.pi_c[0])}, ${p256$1(proof.pi_c[1])}],` +
            `[${inputs}]`;

        return S;
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    var groth16 = /*#__PURE__*/Object.freeze({
        __proto__: null,
        fullProve: groth16FullProve,
        prove: groth16Prove,
        verify: groth16Verify,
        exportSolidityCallData: groth16ExportSolidityCallData
    });

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    function hashToG2(curve, hash) {
        const hashV = new DataView(hash.buffer, hash.byteOffset, hash.byteLength);
        const seed = [];
        for (let i=0; i<8; i++) {
            seed[i] = hashV.getUint32(i*4);
        }

        const rng = new ChaCha(seed);

        const g2_sp = curve.G2.fromRng(rng);

        return g2_sp;
    }

    function getG2sp(curve, persinalization, challenge, g1s, g1sx) {

        const h = blake2bWasm(64);
        const b1 = new Uint8Array([persinalization]);
        h.update(b1);
        h.update(challenge);
        const b3 = curve.G1.toUncompressed(g1s);
        h.update( b3);
        const b4 = curve.G1.toUncompressed(g1sx);
        h.update( b4);
        const hash =h.digest();

        return hashToG2(curve, hash);
    }

    function calculatePubKey(k, curve, personalization, challengeHash, rng ) {
        k.g1_s = curve.G1.toAffine(curve.G1.fromRng(rng));
        k.g1_sx = curve.G1.toAffine(curve.G1.timesFr(k.g1_s, k.prvKey));
        k.g2_sp = curve.G2.toAffine(getG2sp(curve, personalization, challengeHash, k.g1_s, k.g1_sx));
        k.g2_spx = curve.G2.toAffine(curve.G2.timesFr(k.g2_sp, k.prvKey));
        return k;
    }

    function createPTauKey(curve, challengeHash, rng) {
        const key = {
            tau: {},
            alpha: {},
            beta: {}
        };
        key.tau.prvKey = curve.Fr.fromRng(rng);
        key.alpha.prvKey = curve.Fr.fromRng(rng);
        key.beta.prvKey = curve.Fr.fromRng(rng);
        calculatePubKey(key.tau, curve, 0, challengeHash, rng);
        calculatePubKey(key.alpha, curve, 1, challengeHash, rng);
        calculatePubKey(key.beta, curve, 2, challengeHash, rng);
        return key;
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function writePTauHeader(fd, curve, power, ceremonyPower) {
        // Write the header
        ///////////

        if (! ceremonyPower) ceremonyPower = power;
        await fd.writeULE32(1); // Header type
        const pHeaderSize = fd.pos;
        await fd.writeULE64(0); // Temporally set to 0 length

        await fd.writeULE32(curve.F1.n64*8);

        const buff = new Uint8Array(curve.F1.n8);
        Scalar.toRprLE(buff, 0, curve.q, curve.F1.n8);
        await fd.write(buff);
        await fd.writeULE32(power);                    // power
        await fd.writeULE32(ceremonyPower);               // power

        const headerSize = fd.pos - pHeaderSize - 8;

        const oldPos = fd.pos;

        await fd.writeULE64(headerSize, pHeaderSize);

        fd.pos = oldPos;
    }

    async function readPTauHeader(fd, sections) {
        if (!sections[1])  throw new Error(fd.fileName + ": File has no  header");
        if (sections[1].length>1) throw new Error(fd.fileName +": File has more than one header");

        fd.pos = sections[1][0].p;
        const n8 = await fd.readULE32();
        const buff = await fd.read(n8);
        const q = Scalar.fromRprLE(buff);

        const curve = await getCurveFromQ(q);

        if (curve.F1.n64*8 != n8) throw new Error(fd.fileName +": Invalid size");

        const power = await fd.readULE32();
        const ceremonyPower = await fd.readULE32();

        if (fd.pos-sections[1][0].p != sections[1][0].size) throw new Error("Invalid PTau header size");

        return {curve, power, ceremonyPower};
    }


    async function readPtauPubKey(fd, curve, montgomery) {

        const buff = await fd.read(curve.F1.n8*2*6 + curve.F2.n8*2*3);

        return fromPtauPubKeyRpr(buff, 0, curve, montgomery);
    }

    function fromPtauPubKeyRpr(buff, pos, curve, montgomery) {

        const key = {
            tau: {},
            alpha: {},
            beta: {}
        };

        key.tau.g1_s = readG1();
        key.tau.g1_sx = readG1();
        key.alpha.g1_s = readG1();
        key.alpha.g1_sx = readG1();
        key.beta.g1_s = readG1();
        key.beta.g1_sx = readG1();
        key.tau.g2_spx = readG2();
        key.alpha.g2_spx = readG2();
        key.beta.g2_spx = readG2();

        return key;

        function readG1() {
            let p;
            if (montgomery) {
                p = curve.G1.fromRprLEM( buff, pos );
            } else {
                p = curve.G1.fromRprUncompressed( buff, pos );
            }
            pos += curve.G1.F.n8*2;
            return p;
        }

        function readG2() {
            let p;
            if (montgomery) {
                p = curve.G2.fromRprLEM( buff, pos );
            } else {
                p = curve.G2.fromRprUncompressed( buff, pos );
            }
            pos += curve.G2.F.n8*2;
            return p;
        }
    }

    function toPtauPubKeyRpr(buff, pos, curve, key, montgomery) {

        writeG1(key.tau.g1_s);
        writeG1(key.tau.g1_sx);
        writeG1(key.alpha.g1_s);
        writeG1(key.alpha.g1_sx);
        writeG1(key.beta.g1_s);
        writeG1(key.beta.g1_sx);
        writeG2(key.tau.g2_spx);
        writeG2(key.alpha.g2_spx);
        writeG2(key.beta.g2_spx);

        async function writeG1(p) {
            if (montgomery) {
                curve.G1.toRprLEM(buff, pos, p);
            } else {
                curve.G1.toRprUncompressed(buff, pos, p);
            }
            pos += curve.F1.n8*2;
        }

        async function writeG2(p) {
            if (montgomery) {
                curve.G2.toRprLEM(buff, pos, p);
            } else {
                curve.G2.toRprUncompressed(buff, pos, p);
            }
            pos += curve.F2.n8*2;
        }

        return buff;
    }

    async function writePtauPubKey(fd, curve, key, montgomery) {
        const buff = new Uint8Array(curve.F1.n8*2*6 + curve.F2.n8*2*3);
        toPtauPubKeyRpr(buff, 0, curve, key, montgomery);
        await fd.write(buff);
    }

    async function readContribution(fd, curve) {
        const c = {};

        c.tauG1 = await readG1();
        c.tauG2 = await readG2();
        c.alphaG1 = await readG1();
        c.betaG1 = await readG1();
        c.betaG2 = await readG2();
        c.key = await readPtauPubKey(fd, curve, true);
        c.partialHash = await fd.read(216);
        c.nextChallenge = await fd.read(64);
        c.type = await fd.readULE32();

        const buffV  = new Uint8Array(curve.G1.F.n8*2*6+curve.G2.F.n8*2*3);
        toPtauPubKeyRpr(buffV, 0, curve, c.key, false);

        const responseHasher = blake2bWasm(64);
        responseHasher.setPartialHash(c.partialHash);
        responseHasher.update(buffV);
        c.responseHash = responseHasher.digest();

        const paramLength = await fd.readULE32();
        const curPos = fd.pos;
        let lastType =0;
        while (fd.pos-curPos < paramLength) {
            const buffType = await readDV(1);
            if (buffType[0]<= lastType) throw new Error("Parameters in the contribution must be sorted");
            lastType = buffType[0];
            if (buffType[0]==1) {     // Name
                const buffLen = await readDV(1);
                const buffStr = await readDV(buffLen[0]);
                c.name = new TextDecoder().decode(buffStr);
            } else if (buffType[0]==2) {
                const buffExp = await readDV(1);
                c.numIterationsExp = buffExp[0];
            } else if (buffType[0]==3) {
                const buffLen = await readDV(1);
                c.beaconHash = await readDV(buffLen[0]);
            } else {
                throw new Error("Parameter not recognized");
            }
        }
        if (fd.pos != curPos + paramLength) {
            throw new Error("Parametes do not match");
        }

        return c;

        async function readG1() {
            const pBuff = await fd.read(curve.G1.F.n8*2);
            return curve.G1.fromRprLEM( pBuff );
        }

        async function readG2() {
            const pBuff = await fd.read(curve.G2.F.n8*2);
            return curve.G2.fromRprLEM( pBuff );
        }

        async function readDV(n) {
            const b = await fd.read(n);
            return new Uint8Array(b);
        }
    }

    async function readContributions(fd, curve, sections) {
        if (!sections[7])  throw new Error(fd.fileName + ": File has no  contributions");
        if (sections[7][0].length>1) throw new Error(fd.fileName +": File has more than one contributions section");

        fd.pos = sections[7][0].p;
        const nContributions = await fd.readULE32();
        const contributions = [];
        for (let i=0; i<nContributions; i++) {
            const c = await readContribution(fd, curve);
            c.id = i+1;
            contributions.push(c);
        }

        if (fd.pos-sections[7][0].p != sections[7][0].size) throw new Error("Invalid contribution section size");

        return contributions;
    }

    async function writeContribution(fd, curve, contribution) {

        const buffG1 = new Uint8Array(curve.F1.n8*2);
        const buffG2 = new Uint8Array(curve.F2.n8*2);
        await writeG1(contribution.tauG1);
        await writeG2(contribution.tauG2);
        await writeG1(contribution.alphaG1);
        await writeG1(contribution.betaG1);
        await writeG2(contribution.betaG2);
        await writePtauPubKey(fd, curve, contribution.key, true);
        await fd.write(contribution.partialHash);
        await fd.write(contribution.nextChallenge);
        await fd.writeULE32(contribution.type || 0);

        const params = [];
        if (contribution.name) {
            params.push(1);      // Param Name
            const nameData = new TextEncoder("utf-8").encode(contribution.name.substring(0,64));
            params.push(nameData.byteLength);
            for (let i=0; i<nameData.byteLength; i++) params.push(nameData[i]);
        }
        if (contribution.type == 1) {
            params.push(2);      // Param numIterationsExp
            params.push(contribution.numIterationsExp);

            params.push(3);      // Beacon Hash
            params.push(contribution.beaconHash.byteLength);
            for (let i=0; i<contribution.beaconHash.byteLength; i++) params.push(contribution.beaconHash[i]);
        }
        if (params.length>0) {
            const paramsBuff = new Uint8Array(params);
            await fd.writeULE32(paramsBuff.byteLength);
            await fd.write(paramsBuff);
        } else {
            await fd.writeULE32(0);
        }


        async function writeG1(p) {
            curve.G1.toRprLEM(buffG1, 0, p);
            await fd.write(buffG1);
        }

        async function writeG2(p) {
            curve.G2.toRprLEM(buffG2, 0, p);
            await fd.write(buffG2);
        }

    }

    async function writeContributions(fd, curve, contributions) {

        await fd.writeULE32(7); // Header type
        const pContributionsSize = fd.pos;
        await fd.writeULE64(0); // Temporally set to 0 length

        await fd.writeULE32(contributions.length);
        for (let i=0; i< contributions.length; i++) {
            await writeContribution(fd, curve, contributions[i]);
        }
        const contributionsSize = fd.pos - pContributionsSize - 8;

        const oldPos = fd.pos;

        await fd.writeULE64(contributionsSize, pContributionsSize);
        fd.pos = oldPos;
    }

    function calculateFirstChallengeHash(curve, power, logger) {
        if (logger) logger.debug("Calculating First Challenge Hash");

        const hasher = new blake2bWasm(64);

        const vG1 = new Uint8Array(curve.G1.F.n8*2);
        const vG2 = new Uint8Array(curve.G2.F.n8*2);
        curve.G1.toRprUncompressed(vG1, 0, curve.G1.g);
        curve.G2.toRprUncompressed(vG2, 0, curve.G2.g);

        hasher.update(blake2bWasm(64).digest());

        let n;

        n=(2 ** power)*2 -1;
        if (logger) logger.debug("Calculate Initial Hash: tauG1");
        hashBlock(vG1, n);
        n= 2 ** power;
        if (logger) logger.debug("Calculate Initial Hash: tauG2");
        hashBlock(vG2, n);
        if (logger) logger.debug("Calculate Initial Hash: alphaTauG1");
        hashBlock(vG1, n);
        if (logger) logger.debug("Calculate Initial Hash: betaTauG1");
        hashBlock(vG1, n);
        hasher.update(vG2);

        return hasher.digest();

        function hashBlock(buff, n) {
            const blockSize = 500000;
            const nBlocks = Math.floor(n / blockSize);
            const rem = n % blockSize;
            const bigBuff = new Uint8Array(blockSize * buff.byteLength);
            for (let i=0; i<blockSize; i++) {
                bigBuff.set(buff, i*buff.byteLength);
            }
            for (let i=0; i<nBlocks; i++) {
                hasher.update(bigBuff);
                if (logger) logger.debug("Initial hash: " +i*blockSize);
            }
            for (let i=0; i<rem; i++) {
                hasher.update(buff);
            }
        }
    }


    function keyFromBeacon(curve, challengeHash, beaconHash, numIterationsExp) {

        const rng = rngFromBeaconParams(beaconHash, numIterationsExp);

        const key = createPTauKey(curve, challengeHash, rng);

        return key;
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function newAccumulator(curve, power, fileName, logger) {

        await blake2bWasm.ready();

        const fd = await createBinFile(fileName, "ptau", 1, 7);

        await writePTauHeader(fd, curve, power, 0);

        const buffG1 = curve.G1.oneAffine;
        const buffG2 = curve.G2.oneAffine;

        // Write tauG1
        ///////////
        await startWriteSection(fd, 2);
        const nTauG1 = (2 ** power) * 2 -1;
        for (let i=0; i< nTauG1; i++) {
            await fd.write(buffG1);
            if ((logger)&&((i%100000) == 0)&&i) logger.log("tauG1: " + i);
        }
        await endWriteSection(fd);

        // Write tauG2
        ///////////
        await startWriteSection(fd, 3);
        const nTauG2 = (2 ** power);
        for (let i=0; i< nTauG2; i++) {
            await fd.write(buffG2);
            if ((logger)&&((i%100000) == 0)&&i) logger.log("tauG2: " + i);
        }
        await endWriteSection(fd);

        // Write alphaTauG1
        ///////////
        await startWriteSection(fd, 4);
        const nAlfaTauG1 = (2 ** power);
        for (let i=0; i< nAlfaTauG1; i++) {
            await fd.write(buffG1);
            if ((logger)&&((i%100000) == 0)&&i) logger.log("alphaTauG1: " + i);
        }
        await endWriteSection(fd);

        // Write betaTauG1
        ///////////
        await startWriteSection(fd, 5);
        const nBetaTauG1 = (2 ** power);
        for (let i=0; i< nBetaTauG1; i++) {
            await fd.write(buffG1);
            if ((logger)&&((i%100000) == 0)&&i) logger.log("betaTauG1: " + i);
        }
        await endWriteSection(fd);

        // Write betaG2
        ///////////
        await startWriteSection(fd, 6);
        await fd.write(buffG2);
        await endWriteSection(fd);

        // Contributions
        ///////////
        await startWriteSection(fd, 7);
        await fd.writeULE32(0); // 0 Contributions
        await endWriteSection(fd);

        await fd.close();

        const firstChallengeHash = calculateFirstChallengeHash(curve, power, logger);

        if (logger) logger.debug(formatHash(blake2bWasm(64).digest(), "Blank Contribution Hash:"));

        if (logger) logger.info(formatHash(firstChallengeHash, "First Contribution Hash:"));

        return firstChallengeHash;

    }

    // Format of the outpu

    async function exportChallenge(pTauFilename, challengeFilename, logger) {
        await blake2bWasm.ready();
        const {fd: fdFrom, sections} = await readBinFile(pTauFilename, "ptau", 1);

        const {curve, power} = await readPTauHeader(fdFrom, sections);

        const contributions = await readContributions(fdFrom, curve, sections);
        let lastResponseHash, curChallengeHash;
        if (contributions.length == 0) {
            lastResponseHash = blake2bWasm(64).digest();
            curChallengeHash = calculateFirstChallengeHash(curve, power);
        } else {
            lastResponseHash = contributions[contributions.length-1].responseHash;
            curChallengeHash = contributions[contributions.length-1].nextChallenge;
        }

        if (logger) logger.info(formatHash(lastResponseHash, "Last Response Hash: "));

        if (logger) logger.info(formatHash(curChallengeHash, "New Challenge Hash: "));


        const fdTo = await createOverride(challengeFilename);

        const toHash = blake2bWasm(64);
        await fdTo.write(lastResponseHash);
        toHash.update(lastResponseHash);

        await exportSection(2, "G1", (2 ** power) * 2 -1, "tauG1");
        await exportSection(3, "G2", (2 ** power)       , "tauG2");
        await exportSection(4, "G1", (2 ** power)       , "alphaTauG1");
        await exportSection(5, "G1", (2 ** power)       , "betaTauG1");
        await exportSection(6, "G2", 1                  , "betaG2");

        await fdFrom.close();
        await fdTo.close();

        const calcCurChallengeHash = toHash.digest();

        if (!hashIsEqual (curChallengeHash, calcCurChallengeHash)) {
            if (logger) logger.info(formatHash(calcCurChallengeHash, "Calc Curret Challenge Hash: "));

            if (logger) logger.error("PTau file is corrupted. Calculated new challenge hash does not match with the eclared one");
            throw new Error("PTau file is corrupted. Calculated new challenge hash does not match with the eclared one");
        }

        return curChallengeHash;

        async function exportSection(sectionId, groupName, nPoints, sectionName) {
            const G = curve[groupName];
            const sG = G.F.n8*2;
            const nPointsChunk = Math.floor((1<<24)/sG);

            await startReadUniqueSection(fdFrom, sections, sectionId);
            for (let i=0; i< nPoints; i+= nPointsChunk) {
                if (logger) logger.debug(`Exporting ${sectionName}: ${i}/${nPoints}`);
                const n = Math.min(nPoints-i, nPointsChunk);
                let buff;
                buff = await fdFrom.read(n*sG);
                buff = await G.batchLEMtoU(buff);
                await fdTo.write(buff);
                toHash.update(buff);
            }
            await endReadSection(fdFrom);
        }


    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function importResponse(oldPtauFilename, contributionFilename, newPTauFilename, name, importPoints, logger) {

        await blake2bWasm.ready();

        const noHash = new Uint8Array(64);
        for (let i=0; i<64; i++) noHash[i] = 0xFF;

        const {fd: fdOld, sections} = await readBinFile(oldPtauFilename, "ptau", 1);
        const {curve, power} = await readPTauHeader(fdOld, sections);
        const contributions = await readContributions(fdOld, curve, sections);
        const currentContribution = {};

        if (name) currentContribution.name = name;

        const sG1 = curve.F1.n8*2;
        const scG1 = curve.F1.n8; // Compresed size
        const sG2 = curve.F2.n8*2;
        const scG2 = curve.F2.n8; // Compresed size

        const fdResponse = await readExisting(contributionFilename);

        if  (fdResponse.totalSize !=
            64 +                            // Old Hash
            ((2 ** power)*2-1)*scG1 +
            (2 ** power)*scG2 +
            (2 ** power)*scG1 +
            (2 ** power)*scG1 +
            scG2 +
            sG1*6 + sG2*3)
            throw new Error("Size of the contribution is invalid");

        let lastChallengeHash;

        if (contributions.length>0) {
            lastChallengeHash = contributions[contributions.length-1].nextChallenge;
        } else {
            lastChallengeHash = calculateFirstChallengeHash(curve, power, logger);
        }

        const fdNew = await createBinFile(newPTauFilename, "ptau", 1, importPoints ? 7: 2);
        await writePTauHeader(fdNew, curve, power);

        const contributionPreviousHash = await fdResponse.read(64);

        if (hashIsEqual(noHash,lastChallengeHash)) {
            lastChallengeHash = contributionPreviousHash;
            contributions[contributions.length-1].nextChallenge = lastChallengeHash;
        }

        if(!hashIsEqual(contributionPreviousHash,lastChallengeHash))
            throw new Error("Wrong contribution. this contribution is not based on the previus hash");

        const hasherResponse = new blake2bWasm(64);
        hasherResponse.update(contributionPreviousHash);

        const startSections = [];
        let res;
        res = await processSection(fdResponse, fdNew, "G1", 2, (2 ** power) * 2 -1, [1], "tauG1");
        currentContribution.tauG1 = res[0];
        res = await processSection(fdResponse, fdNew, "G2", 3, (2 ** power)       , [1], "tauG2");
        currentContribution.tauG2 = res[0];
        res = await processSection(fdResponse, fdNew, "G1", 4, (2 ** power)       , [0], "alphaG1");
        currentContribution.alphaG1 = res[0];
        res = await processSection(fdResponse, fdNew, "G1", 5, (2 ** power)       , [0], "betaG1");
        currentContribution.betaG1 = res[0];
        res = await processSection(fdResponse, fdNew, "G2", 6, 1                  , [0], "betaG2");
        currentContribution.betaG2 = res[0];

        currentContribution.partialHash = hasherResponse.getPartialHash();


        const buffKey = await fdResponse.read(curve.F1.n8*2*6+curve.F2.n8*2*3);

        currentContribution.key = fromPtauPubKeyRpr(buffKey, 0, curve, false);

        hasherResponse.update(new Uint8Array(buffKey));
        const hashResponse = hasherResponse.digest();

        if (logger) logger.info(formatHash(hashResponse, "Contribution Response Hash imported: "));

        if (importPoints) {
            const nextChallengeHasher = new blake2bWasm(64);
            nextChallengeHasher.update(hashResponse);

            await hashSection(nextChallengeHasher, fdNew, "G1", 2, (2 ** power) * 2 -1, "tauG1", logger);
            await hashSection(nextChallengeHasher, fdNew, "G2", 3, (2 ** power)       , "tauG2", logger);
            await hashSection(nextChallengeHasher, fdNew, "G1", 4, (2 ** power)       , "alphaTauG1", logger);
            await hashSection(nextChallengeHasher, fdNew, "G1", 5, (2 ** power)       , "betaTauG1", logger);
            await hashSection(nextChallengeHasher, fdNew, "G2", 6, 1                  , "betaG2", logger);

            currentContribution.nextChallenge = nextChallengeHasher.digest();

            if (logger) logger.info(formatHash(currentContribution.nextChallenge, "Next Challenge Hash: "));
        } else {
            currentContribution.nextChallenge = noHash;
        }

        contributions.push(currentContribution);

        await writeContributions(fdNew, curve, contributions);

        await fdResponse.close();
        await fdNew.close();
        await fdOld.close();

        return currentContribution.nextChallenge;

        async function processSection(fdFrom, fdTo, groupName, sectionId, nPoints, singularPointIndexes, sectionName) {
            if (importPoints) {
                return await processSectionImportPoints(fdFrom, fdTo, groupName, sectionId, nPoints, singularPointIndexes, sectionName);
            } else {
                return await processSectionNoImportPoints(fdFrom, fdTo, groupName, sectionId, nPoints, singularPointIndexes, sectionName);
            }
        }

        async function processSectionImportPoints(fdFrom, fdTo, groupName, sectionId, nPoints, singularPointIndexes, sectionName) {

            const G = curve[groupName];
            const scG = G.F.n8;
            const sG = G.F.n8*2;

            const singularPoints = [];

            await startWriteSection(fdTo, sectionId);
            const nPointsChunk = Math.floor((1<<24)/sG);

            startSections[sectionId] = fdTo.pos;

            for (let i=0; i< nPoints; i += nPointsChunk) {
                if (logger) logger.debug(`Importing ${sectionName}: ${i}/${nPoints}`);
                const n = Math.min(nPoints-i, nPointsChunk);

                const buffC = await fdFrom.read(n * scG);
                hasherResponse.update(buffC);

                const buffLEM = await G.batchCtoLEM(buffC);

                await fdTo.write(buffLEM);
                for (let j=0; j<singularPointIndexes.length; j++) {
                    const sp = singularPointIndexes[j];
                    if ((sp >=i) && (sp < i+n)) {
                        const P = G.fromRprLEM(buffLEM, (sp-i)*sG);
                        singularPoints.push(P);
                    }
                }
            }

            await endWriteSection(fdTo);

            return singularPoints;
        }


        async function processSectionNoImportPoints(fdFrom, fdTo, groupName, sectionId, nPoints, singularPointIndexes, sectionName) {

            const G = curve[groupName];
            const scG = G.F.n8;

            const singularPoints = [];

            const nPointsChunk = Math.floor((1<<24)/scG);

            for (let i=0; i< nPoints; i += nPointsChunk) {
                if (logger) logger.debug(`Importing ${sectionName}: ${i}/${nPoints}`);
                const n = Math.min(nPoints-i, nPointsChunk);

                const buffC = await fdFrom.read(n * scG);
                hasherResponse.update(buffC);

                for (let j=0; j<singularPointIndexes.length; j++) {
                    const sp = singularPointIndexes[j];
                    if ((sp >=i) && (sp < i+n)) {
                        const P = G.fromRprCompressed(buffC, (sp-i)*scG);
                        singularPoints.push(P);
                    }
                }
            }

            return singularPoints;
        }


        async function hashSection(nextChallengeHasher, fdTo, groupName, sectionId, nPoints, sectionName, logger) {

            const G = curve[groupName];
            const sG = G.F.n8*2;
            const nPointsChunk = Math.floor((1<<24)/sG);

            const oldPos = fdTo.pos;
            fdTo.pos = startSections[sectionId];

            for (let i=0; i< nPoints; i += nPointsChunk) {
                if (logger) logger.debug(`Hashing ${sectionName}: ${i}/${nPoints}`);
                const n = Math.min(nPoints-i, nPointsChunk);

                const buffLEM = await fdTo.read(n * sG);

                const buffU = await G.batchLEMtoU(buffLEM);

                nextChallengeHasher.update(buffU);
            }

            fdTo.pos = oldPos;
        }

    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */
    const sameRatio$1 = sameRatio$2;

    async function verifyContribution(curve, cur, prev, logger) {
        let sr;
        if (cur.type == 1) {    // Verify the beacon.
            const beaconKey = keyFromBeacon(curve, prev.nextChallenge, cur.beaconHash, cur.numIterationsExp);

            if (!curve.G1.eq(cur.key.tau.g1_s, beaconKey.tau.g1_s)) {
                if (logger) logger.error(`BEACON key (tauG1_s) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G1.eq(cur.key.tau.g1_sx, beaconKey.tau.g1_sx)) {
                if (logger) logger.error(`BEACON key (tauG1_sx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G2.eq(cur.key.tau.g2_spx, beaconKey.tau.g2_spx)) {
                if (logger) logger.error(`BEACON key (tauG2_spx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }

            if (!curve.G1.eq(cur.key.alpha.g1_s, beaconKey.alpha.g1_s)) {
                if (logger) logger.error(`BEACON key (alphaG1_s) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G1.eq(cur.key.alpha.g1_sx, beaconKey.alpha.g1_sx)) {
                if (logger) logger.error(`BEACON key (alphaG1_sx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G2.eq(cur.key.alpha.g2_spx, beaconKey.alpha.g2_spx)) {
                if (logger) logger.error(`BEACON key (alphaG2_spx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }

            if (!curve.G1.eq(cur.key.beta.g1_s, beaconKey.beta.g1_s)) {
                if (logger) logger.error(`BEACON key (betaG1_s) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G1.eq(cur.key.beta.g1_sx, beaconKey.beta.g1_sx)) {
                if (logger) logger.error(`BEACON key (betaG1_sx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
            if (!curve.G2.eq(cur.key.beta.g2_spx, beaconKey.beta.g2_spx)) {
                if (logger) logger.error(`BEACON key (betaG2_spx) is not generated correctly in challenge #${cur.id}  ${cur.name || ""}` );
                return false;
            }
        }

        cur.key.tau.g2_sp = curve.G2.toAffine(getG2sp(curve, 0, prev.nextChallenge, cur.key.tau.g1_s, cur.key.tau.g1_sx));
        cur.key.alpha.g2_sp = curve.G2.toAffine(getG2sp(curve, 1, prev.nextChallenge, cur.key.alpha.g1_s, cur.key.alpha.g1_sx));
        cur.key.beta.g2_sp = curve.G2.toAffine(getG2sp(curve, 2, prev.nextChallenge, cur.key.beta.g1_s, cur.key.beta.g1_sx));

        sr = await sameRatio$1(curve, cur.key.tau.g1_s, cur.key.tau.g1_sx, cur.key.tau.g2_sp, cur.key.tau.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID key (tau) in challenge #"+cur.id);
            return false;
        }

        sr = await sameRatio$1(curve, cur.key.alpha.g1_s, cur.key.alpha.g1_sx, cur.key.alpha.g2_sp, cur.key.alpha.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID key (alpha) in challenge #"+cur.id);
            return false;
        }

        sr = await sameRatio$1(curve, cur.key.beta.g1_s, cur.key.beta.g1_sx, cur.key.beta.g2_sp, cur.key.beta.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID key (beta) in challenge #"+cur.id);
            return false;
        }

        sr = await sameRatio$1(curve, prev.tauG1, cur.tauG1, cur.key.tau.g2_sp, cur.key.tau.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID tau*G1. challenge #"+cur.id+" It does not follow the previous contribution");
            return false;
        }

        sr = await sameRatio$1(curve,  cur.key.tau.g1_s, cur.key.tau.g1_sx, prev.tauG2, cur.tauG2);
        if (sr !== true) {
            if (logger) logger.error("INVALID tau*G2. challenge #"+cur.id+" It does not follow the previous contribution");
            return false;
        }

        sr = await sameRatio$1(curve, prev.alphaG1, cur.alphaG1, cur.key.alpha.g2_sp, cur.key.alpha.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID alpha*G1. challenge #"+cur.id+" It does not follow the previous contribution");
            return false;
        }

        sr = await sameRatio$1(curve, prev.betaG1, cur.betaG1, cur.key.beta.g2_sp, cur.key.beta.g2_spx);
        if (sr !== true) {
            if (logger) logger.error("INVALID beta*G1. challenge #"+cur.id+" It does not follow the previous contribution");
            return false;
        }

        sr = await sameRatio$1(curve,  cur.key.beta.g1_s, cur.key.beta.g1_sx, prev.betaG2, cur.betaG2);
        if (sr !== true) {
            if (logger) logger.error("INVALID beta*G2. challenge #"+cur.id+"It does not follow the previous contribution");
            return false;
        }

        if (logger) logger.info("Powers Of tau file OK!");
        return true;
    }

    async function verify(tauFilename, logger) {
        let sr;
        await blake2bWasm.ready();

        const {fd, sections} = await readBinFile(tauFilename, "ptau", 1);
        const {curve, power, ceremonyPower} = await readPTauHeader(fd, sections);
        const contrs = await readContributions(fd, curve, sections);

        if (logger) logger.debug("power: 2**" + power);
        // Verify Last contribution

        if (logger) logger.debug("Computing initial contribution hash");
        const initialContribution = {
            tauG1: curve.G1.g,
            tauG2: curve.G2.g,
            alphaG1: curve.G1.g,
            betaG1: curve.G1.g,
            betaG2: curve.G2.g,
            nextChallenge: calculateFirstChallengeHash(curve, ceremonyPower, logger),
            responseHash: blake2bWasm(64).digest()
        };

        if (contrs.length == 0) {
            if (logger) logger.error("This file has no contribution! It cannot be used in production");
            return false;
        }

        let prevContr;
        if (contrs.length>1) {
            prevContr = contrs[contrs.length-2];
        } else {
            prevContr = initialContribution;
        }
        const curContr = contrs[contrs.length-1];
        if (logger) logger.debug("Validating contribution #"+contrs[contrs.length-1].id);
        const res = await verifyContribution(curve, curContr, prevContr, logger);
        if (!res) return false;


        const nextContributionHasher = blake2bWasm(64);
        nextContributionHasher.update(curContr.responseHash);

        // Verify powers and compute nextChallengeHash

        // await test();

        // Verify Section tau*G1
        if (logger) logger.debug("Verifying powers in tau*G1 section");
        const rTau1 = await processSection(2, "G1", "tauG1", (2 ** power)*2-1, [0, 1], logger);
        sr = await sameRatio$1(curve, rTau1.R1, rTau1.R2, curve.G2.g, curContr.tauG2);
        if (sr !== true) {
            if (logger) logger.error("tauG1 section. Powers do not match");
            return false;
        }
        if (!curve.G1.eq(curve.G1.g, rTau1.singularPoints[0])) {
            if (logger) logger.error("First element of tau*G1 section must be the generator");
            return false;
        }
        if (!curve.G1.eq(curContr.tauG1, rTau1.singularPoints[1])) {
            if (logger) logger.error("Second element of tau*G1 section does not match the one in the contribution section");
            return false;
        }

        // await test();

        // Verify Section tau*G2
        if (logger) logger.debug("Verifying powers in tau*G2 section");
        const rTau2 = await processSection(3, "G2", "tauG2", 2 ** power, [0, 1],  logger);
        sr = await sameRatio$1(curve, curve.G1.g, curContr.tauG1, rTau2.R1, rTau2.R2);
        if (sr !== true) {
            if (logger) logger.error("tauG2 section. Powers do not match");
            return false;
        }
        if (!curve.G2.eq(curve.G2.g, rTau2.singularPoints[0])) {
            if (logger) logger.error("First element of tau*G2 section must be the generator");
            return false;
        }
        if (!curve.G2.eq(curContr.tauG2, rTau2.singularPoints[1])) {
            if (logger) logger.error("Second element of tau*G2 section does not match the one in the contribution section");
            return false;
        }

        // Verify Section alpha*tau*G1
        if (logger) logger.debug("Verifying powers in alpha*tau*G1 section");
        const rAlphaTauG1 = await processSection(4, "G1", "alphatauG1", 2 ** power, [0], logger);
        sr = await sameRatio$1(curve, rAlphaTauG1.R1, rAlphaTauG1.R2, curve.G2.g, curContr.tauG2);
        if (sr !== true) {
            if (logger) logger.error("alphaTauG1 section. Powers do not match");
            return false;
        }
        if (!curve.G1.eq(curContr.alphaG1, rAlphaTauG1.singularPoints[0])) {
            if (logger) logger.error("First element of alpha*tau*G1 section (alpha*G1) does not match the one in the contribution section");
            return false;
        }

        // Verify Section beta*tau*G1
        if (logger) logger.debug("Verifying powers in beta*tau*G1 section");
        const rBetaTauG1 = await processSection(5, "G1", "betatauG1", 2 ** power, [0], logger);
        sr = await sameRatio$1(curve, rBetaTauG1.R1, rBetaTauG1.R2, curve.G2.g, curContr.tauG2);
        if (sr !== true) {
            if (logger) logger.error("betaTauG1 section. Powers do not match");
            return false;
        }
        if (!curve.G1.eq(curContr.betaG1, rBetaTauG1.singularPoints[0])) {
            if (logger) logger.error("First element of beta*tau*G1 section (beta*G1) does not match the one in the contribution section");
            return false;
        }

        //Verify Beta G2
        const betaG2 = await processSectionBetaG2(logger);
        if (!curve.G2.eq(curContr.betaG2, betaG2)) {
            if (logger) logger.error("betaG2 element in betaG2 section does not match the one in the contribution section");
            return false;
        }


        const nextContributionHash = nextContributionHasher.digest();

        // Check the nextChallengeHash
        if (power == ceremonyPower) {
            if (!hashIsEqual(nextContributionHash,curContr.nextChallenge)) {
                if (logger) logger.error("Hash of the values does not match the next challenge of the last contributor in the contributions section");
                return false;
            }
        }

        if (logger) logger.info(formatHash(nextContributionHash, "Next challenge hash: "));

        // Verify Previous contributions

        printContribution(curContr, prevContr);
        for (let i = contrs.length-2; i>=0; i--) {
            const curContr = contrs[i];
            const prevContr =  (i>0) ? contrs[i-1] : initialContribution;
            const res = await verifyContribution(curve, curContr, prevContr, logger);
            if (!res) return false;
            printContribution(curContr, prevContr);
        }
        if (logger) logger.info("-----------------------------------------------------");

        if ((!sections[12]) || (!sections[13]) || (!sections[14]) || (!sections[15])) {
            if (logger) logger.warn(
                "this file does not contain phase2 precalculated values. Please run: \n" +
                "   snarkjs \"powersoftau preparephase2\" to prepare this file to be used in the phase2 ceremony."
            );
        } else {
            let res;
            res = await verifyLagrangeEvaluations("G1", 2, 12, "tauG1", logger);
            if (!res) return false;
            res = await verifyLagrangeEvaluations("G2", 3, 13, "tauG2", logger);
            if (!res) return false;
            res = await verifyLagrangeEvaluations("G1", 4, 14, "alphaTauG1", logger);
            if (!res) return false;
            res = await verifyLagrangeEvaluations("G1", 5, 15, "betaTauG1", logger);
            if (!res) return false;
        }

        await fd.close();

        if (logger) logger.info("Powers of Tau Ok!");

        return true;

        function printContribution(curContr, prevContr) {
            if (!logger) return;
            logger.info("-----------------------------------------------------");
            logger.info(`Contribution #${curContr.id}: ${curContr.name ||""}`);

            logger.info(formatHash(curContr.nextChallenge, "Next Challenge: "));

            const buffV  = new Uint8Array(curve.G1.F.n8*2*6+curve.G2.F.n8*2*3);
            toPtauPubKeyRpr(buffV, 0, curve, curContr.key, false);

            const responseHasher = blake2bWasm(64);
            responseHasher.setPartialHash(curContr.partialHash);
            responseHasher.update(buffV);
            const responseHash = responseHasher.digest();

            logger.info(formatHash(responseHash, "Response Hash:"));

            logger.info(formatHash(prevContr.nextChallenge, "Response Hash:"));

            if (curContr.type == 1) {
                logger.info(`Beacon generator: ${byteArray2hex(curContr.beaconHash)}`);
                logger.info(`Beacon iterations Exp: ${curContr.numIterationsExp}`);
            }

        }

        async function processSectionBetaG2(logger) {
            const G = curve.G2;
            const sG = G.F.n8*2;
            const buffUv = new Uint8Array(sG);

            if (!sections[6])  {
                logger.error("File has no BetaG2 section");
                throw new Error("File has no BetaG2 section");
            }
            if (sections[6].length>1) {
                logger.error("File has no BetaG2 section");
                throw new Error("File has more than one GetaG2 section");
            }
            fd.pos = sections[6][0].p;

            const buff = await fd.read(sG);
            const P = G.fromRprLEM(buff);

            G.toRprUncompressed(buffUv, 0, P);
            nextContributionHasher.update(buffUv);

            return P;
        }

        async function processSection(idSection, groupName, sectionName, nPoints, singularPointIndexes, logger) {
            const MAX_CHUNK_SIZE = 1<<16;
            const G = curve[groupName];
            const sG = G.F.n8*2;
            await startReadUniqueSection(fd, sections, idSection);

            const singularPoints = [];

            let R1 = G.zero;
            let R2 = G.zero;

            let lastBase = G.zero;

            for (let i=0; i<nPoints; i += MAX_CHUNK_SIZE) {
                if (logger) logger.debug(`points relations: ${sectionName}: ${i}/${nPoints} `);
                const n = Math.min(nPoints - i, MAX_CHUNK_SIZE);
                const bases = await fd.read(n*sG);

                const basesU = await G.batchLEMtoU(bases);
                nextContributionHasher.update(basesU);

                const scalars = new Uint8Array(4*(n-1));
                crypto.randomFillSync(scalars);


                if (i>0) {
                    const firstBase = G.fromRprLEM(bases, 0);
                    const r = crypto.randomBytes(4).readUInt32BE(0, true);

                    R1 = G.add(R1, G.timesScalar(lastBase, r));
                    R2 = G.add(R2, G.timesScalar(firstBase, r));
                }

                const r1 = await G.multiExpAffine(bases.slice(0, (n-1)*sG), scalars);
                const r2 = await G.multiExpAffine(bases.slice(sG), scalars);

                R1 = G.add(R1, r1);
                R2 = G.add(R2, r2);

                lastBase = G.fromRprLEM( bases, (n-1)*sG);

                for (let j=0; j<singularPointIndexes.length; j++) {
                    const sp = singularPointIndexes[j];
                    if ((sp >=i) && (sp < i+n)) {
                        const P = G.fromRprLEM(bases, (sp-i)*sG);
                        singularPoints.push(P);
                    }
                }

            }
            await endReadSection(fd);

            return {
                R1: R1,
                R2: R2,
                singularPoints: singularPoints
            };

        }

        async function verifyLagrangeEvaluations(gName, tauSection, lagrangeSection, sectionName, logger) {

            if (logger) logger.debug(`Verifying phase2 calculated values ${sectionName}...`);
            const G = curve[gName];
            const sG = G.F.n8*2;

            const seed= new Array(8);
            for (let i=0; i<8; i++) {
                seed[i] = crypto.randomBytes(4).readUInt32BE(0, true);
            }

            for (let p=0; p<= power; p ++) {
                const res = await verifyPower(p);
                if (!res) return false;
            }

            if (tauSection == 2) {
                const res = await verifyPower(power+1);
                if (!res) return false;
            }

            return true;

            async function verifyPower(p) {
                if (logger) logger.debug(`Power ${p}...`);
                const n8r = curve.Fr.n8;
                const nPoints = 2 ** p;
                let buff_r = new Uint32Array(nPoints);
                let buffG;

                let rng = new ChaCha(seed);

                if (logger) logger.debug(`Creating random numbers Powers${p}...`);
                for (let i=0; i<nPoints; i++) {
                    if ((p == power+1)&&(i == nPoints-1)) {
                        buff_r[i] = 0;
                    } else {
                        buff_r[i] = rng.nextU32();
                    }
                }

                buff_r = new Uint8Array(buff_r.buffer, buff_r.byteOffset, buff_r.byteLength);

                if (logger) logger.debug(`reading points Powers${p}...`);
                await startReadUniqueSection(fd, sections, tauSection);
                buffG = new BigBuffer(nPoints*sG);
                if (p == power+1) {
                    await fd.readToBuffer(buffG, 0, (nPoints-1)*sG);
                    buffG.set(curve.G1.zeroAffine, (nPoints-1)*sG);
                } else {
                    await fd.readToBuffer(buffG, 0, nPoints*sG);
                }
                await endReadSection(fd, true);

                const resTau = await G.multiExpAffine(buffG, buff_r, logger, sectionName + "_" + p);

                buff_r = new BigBuffer(nPoints * n8r);

                rng = new ChaCha(seed);

                const buff4 = new Uint8Array(4);
                const buff4V = new DataView(buff4.buffer);

                if (logger) logger.debug(`Creating random numbers Powers${p}...`);
                for (let i=0; i<nPoints; i++) {
                    if ((i != nPoints-1) || (p != power+1)) {
                        buff4V.setUint32(0, rng.nextU32(), true);
                        buff_r.set(buff4, i*n8r);
                    }
                }

                if (logger) logger.debug(`batchToMontgomery ${p}...`);
                buff_r = await curve.Fr.batchToMontgomery(buff_r);
                if (logger) logger.debug(`fft ${p}...`);
                buff_r = await curve.Fr.fft(buff_r);
                if (logger) logger.debug(`batchFromMontgomery ${p}...`);
                buff_r = await curve.Fr.batchFromMontgomery(buff_r);

                if (logger) logger.debug(`reading points Lagrange${p}...`);
                await startReadUniqueSection(fd, sections, lagrangeSection);
                fd.pos += sG*((2 ** p)-1);
                await fd.readToBuffer(buffG, 0, nPoints*sG);
                await endReadSection(fd, true);

                const resLagrange = await G.multiExpAffine(buffG, buff_r, logger, sectionName + "_" + p + "_transformed");

                if (!G.eq(resTau, resLagrange)) {
                    if (logger) logger.error("Phase2 caclutation does not match with powers of tau");
                    return false;
                }

                return true;
            }
        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    /*
        This function creates a new section in the fdTo file with id idSection.
        It multiplies the pooints in fdFrom by first, first*inc, first*inc^2, ....
        nPoint Times.
        It also updates the newChallengeHasher with the new points
    */

    async function applyKeyToSection(zkeyFileNameOld, maxZKeyVersion, zkeyFileNameNew, idSection, curve, groupName, first, inc, sectionName, logger) {
        const MAX_CHUNK_SIZE = 1 << 16;
        const G = curve[groupName];
        const sG = G.F.n8*2;

        const fdOld = await startReadSectionFile(zkeyFileNameOld, idSection, maxZKeyVersion);
        const fdNew = await startWriteSectionFile(zkeyFileNameNew, idSection);

        const nPoints = fdOld.readingSection.size / sG;
        let t = first;
        for (let i=0; i<nPoints; i += MAX_CHUNK_SIZE) {
            if (logger) logger.debug(`Applying key: ${sectionName}: ${i}/${nPoints}`);
            const n= Math.min(nPoints - i, MAX_CHUNK_SIZE);
            let buff;
            buff = await fdOld.read(n*sG);
            buff = await G.batchApplyKey(buff, t, inc);
            await fdNew.write(buff);
            t = curve.Fr.mul(t, curve.Fr.exp(inc, n));
        }

        await endWriteSectionFile(fdNew);
        await endReadSectionFile(fdOld);
    }



    async function applyKeyToChallengeSection(fdOld, fdNew, responseHasher, curve, groupName, nPoints, first, inc, formatOut, sectionName, logger) {
        const G = curve[groupName];
        const sG = G.F.n8*2;
        const chunkSize = Math.floor((1<<20) / sG);   // 128Mb chunks
        let t = first;
        for (let i=0 ; i<nPoints ; i+= chunkSize) {
            if (logger) logger.debug(`Applying key ${sectionName}: ${i}/${nPoints}`);
            const n= Math.min(nPoints-i, chunkSize );
            const buffInU = await fdOld.read(n * sG);
            const buffInLEM = await G.batchUtoLEM(buffInU);
            const buffOutLEM = await G.batchApplyKey(buffInLEM, t, inc);
            let buffOut;
            if (formatOut == "COMPRESSED") {
                buffOut = await G.batchLEMtoC(buffOutLEM);
            } else {
                buffOut = await G.batchLEMtoU(buffOutLEM);
            }

            if (responseHasher) responseHasher.update(buffOut);
            await fdNew.write(buffOut);
            t = curve.Fr.mul(t, curve.Fr.exp(inc, n));
        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function challengeContribute(curve, challengeFilename, responesFileName, entropy, logger) {
        await blake2bWasm.ready();

        const fdFrom = await readExisting(challengeFilename);


        const sG1 = curve.F1.n64*8*2;
        const sG2 = curve.F2.n64*8*2;
        const domainSize = (fdFrom.totalSize + sG1 - 64 - sG2) / (4*sG1 + sG2);
        let e = domainSize;
        let power = 0;
        while (e>1) {
            e = e /2;
            power += 1;
        }

        if (2 ** power != domainSize) throw new Error("Invalid file size");
        if (logger) logger.debug("Power to tau size: "+power);

        const rng = await getRandomRng(entropy);

        const fdTo = await createOverride(responesFileName);

        // Calculate the hash
        const challengeHasher = blake2bWasm(64);
        for (let i=0; i<fdFrom.totalSize; i+= fdFrom.pageSize) {
            if (logger) logger.debug(`Hashing challenge ${i}/${fdFrom.totalSize}`);
            const s = Math.min(fdFrom.totalSize - i, fdFrom.pageSize);
            const buff = await fdFrom.read(s);
            challengeHasher.update(buff);
        }

        const claimedHash = await fdFrom.read(64, 0);
        if (logger) logger.info(formatHash(claimedHash, "Claimed Previous Response Hash: "));

        const challengeHash = challengeHasher.digest();
        if (logger) logger.info(formatHash(challengeHash, "Current Challenge Hash: "));

        const key = createPTauKey(curve, challengeHash, rng);

        if (logger) {
            ["tau", "alpha", "beta"].forEach( (k) => {
                logger.debug(k + ".g1_s: " + curve.G1.toString(key[k].g1_s, 16));
                logger.debug(k + ".g1_sx: " + curve.G1.toString(key[k].g1_sx, 16));
                logger.debug(k + ".g2_sp: " + curve.G2.toString(key[k].g2_sp, 16));
                logger.debug(k + ".g2_spx: " + curve.G2.toString(key[k].g2_spx, 16));
                logger.debug("");
            });
        }

        const responseHasher = blake2bWasm(64);

        await fdTo.write(challengeHash);
        responseHasher.update(challengeHash);

        await applyKeyToChallengeSection(fdFrom, fdTo, responseHasher, curve, "G1", (2 ** power)*2-1, curve.Fr.one    , key.tau.prvKey, "COMPRESSED", "tauG1"     , logger );
        await applyKeyToChallengeSection(fdFrom, fdTo, responseHasher, curve, "G2", (2 ** power)    , curve.Fr.one    , key.tau.prvKey, "COMPRESSED", "tauG2"     , logger );
        await applyKeyToChallengeSection(fdFrom, fdTo, responseHasher, curve, "G1", (2 ** power)    , key.alpha.prvKey, key.tau.prvKey, "COMPRESSED", "alphaTauG1", logger );
        await applyKeyToChallengeSection(fdFrom, fdTo, responseHasher, curve, "G1", (2 ** power)    , key.beta.prvKey , key.tau.prvKey, "COMPRESSED", "betaTauG1" , logger );
        await applyKeyToChallengeSection(fdFrom, fdTo, responseHasher, curve, "G2", 1             , key.beta.prvKey , key.tau.prvKey, "COMPRESSED", "betaTauG2" , logger );

        // Write and hash key
        const buffKey = new Uint8Array(curve.F1.n8*2*6+curve.F2.n8*2*3);
        toPtauPubKeyRpr(buffKey, 0, curve, key, false);
        await fdTo.write(buffKey);
        responseHasher.update(buffKey);
        const responseHash = responseHasher.digest();
        if (logger) logger.info(formatHash(responseHash, "Contribution Response Hash: "));

        await fdTo.close();
        await fdFrom.close();
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function beacon$1(oldPtauFilename, newPTauFilename, name,  beaconHashStr,numIterationsExp, logger) {
        const beaconHash = hex2ByteArray(beaconHashStr);
        if (   (beaconHash.byteLength == 0)
            || (beaconHash.byteLength*2 !=beaconHashStr.length))
        {
            if (logger) logger.error("Invalid Beacon Hash. (It must be a valid hexadecimal sequence)");
            return false;
        }
        if (beaconHash.length>=256) {
            if (logger) logger.error("Maximum lenght of beacon hash is 255 bytes");
            return false;
        }

        numIterationsExp = parseInt(numIterationsExp);
        if ((numIterationsExp<10)||(numIterationsExp>63)) {
            if (logger) logger.error("Invalid numIterationsExp. (Must be between 10 and 63)");
            return false;
        }


        await blake2bWasm.ready();

        const {fd: fdOld, sections} = await readBinFile(oldPtauFilename, "ptau", 1);
        const {curve, power, ceremonyPower} = await readPTauHeader(fdOld, sections);
        if (power != ceremonyPower) {
            if (logger) logger.error("This file has been reduced. You cannot contribute into a reduced file.");
            return false;
        }
        if (sections[12]) {
            if (logger) logger.warn("Contributing into a file that has phase2 calculated. You will have to prepare phase2 again.");
        }
        const contributions = await readContributions(fdOld, curve, sections);
        const curContribution = {
            name: name,
            type: 1, // Beacon
            numIterationsExp: numIterationsExp,
            beaconHash: beaconHash
        };

        let lastChallengeHash;

        if (contributions.length>0) {
            lastChallengeHash = contributions[contributions.length-1].nextChallenge;
        } else {
            lastChallengeHash = calculateFirstChallengeHash(curve, power, logger);
        }

        curContribution.key = keyFromBeacon(curve, lastChallengeHash, beaconHash, numIterationsExp);

        const responseHasher = new blake2bWasm(64);
        responseHasher.update(lastChallengeHash);

        const fdNew = await createBinFile(newPTauFilename, "ptau", 1, 7);
        await writePTauHeader(fdNew, curve, power);

        const startSections = [];

        let firstPoints;
        firstPoints = await processSection(2, "G1",  (2 ** power) * 2 -1, curve.Fr.e(1), curContribution.key.tau.prvKey, "tauG1", logger );
        curContribution.tauG1 = firstPoints[1];
        firstPoints = await processSection(3, "G2",  (2 ** power) , curve.Fr.e(1), curContribution.key.tau.prvKey, "tauG2", logger );
        curContribution.tauG2 = firstPoints[1];
        firstPoints = await processSection(4, "G1",  (2 ** power) , curContribution.key.alpha.prvKey, curContribution.key.tau.prvKey, "alphaTauG1", logger );
        curContribution.alphaG1 = firstPoints[0];
        firstPoints = await processSection(5, "G1",  (2 ** power) , curContribution.key.beta.prvKey, curContribution.key.tau.prvKey, "betaTauG1", logger );
        curContribution.betaG1 = firstPoints[0];
        firstPoints = await processSection(6, "G2",  1, curContribution.key.beta.prvKey, curContribution.key.tau.prvKey, "betaTauG2", logger );
        curContribution.betaG2 = firstPoints[0];

        curContribution.partialHash = responseHasher.getPartialHash();

        const buffKey = new Uint8Array(curve.F1.n8*2*6+curve.F2.n8*2*3);

        toPtauPubKeyRpr(buffKey, 0, curve, curContribution.key, false);

        responseHasher.update(new Uint8Array(buffKey));
        const hashResponse = responseHasher.digest();

        if (logger) logger.info(formatHash(hashResponse, "Contribution Response Hash imported: "));

        const nextChallengeHasher = new blake2bWasm(64);
        nextChallengeHasher.update(hashResponse);

        await hashSection(fdNew, "G1", 2, (2 ** power) * 2 -1, "tauG1", logger);
        await hashSection(fdNew, "G2", 3, (2 ** power)       , "tauG2", logger);
        await hashSection(fdNew, "G1", 4, (2 ** power)       , "alphaTauG1", logger);
        await hashSection(fdNew, "G1", 5, (2 ** power)       , "betaTauG1", logger);
        await hashSection(fdNew, "G2", 6, 1                  , "betaG2", logger);

        curContribution.nextChallenge = nextChallengeHasher.digest();

        if (logger) logger.info(formatHash(curContribution.nextChallenge, "Next Challenge Hash: "));

        contributions.push(curContribution);

        await writeContributions(fdNew, curve, contributions);

        await fdOld.close();
        await fdNew.close();

        return hashResponse;

        async function processSection(sectionId, groupName, NPoints, first, inc, sectionName, logger) {
            const res = [];
            fdOld.pos = sections[sectionId][0].p;

            await startWriteSection(fdNew, sectionId);

            startSections[sectionId] = fdNew.pos;

            const G = curve[groupName];
            const sG = G.F.n8*2;
            const chunkSize = Math.floor((1<<20) / sG);   // 128Mb chunks
            let t = first;
            for (let i=0 ; i<NPoints ; i+= chunkSize) {
                if (logger) logger.debug(`applying key${sectionName}: ${i}/${NPoints}`);
                const n= Math.min(NPoints-i, chunkSize );
                const buffIn = await fdOld.read(n * sG);
                const buffOutLEM = await G.batchApplyKey(buffIn, t, inc);

                /* Code to test the case where we don't have the 2^m-2 component
                if (sectionName== "tauG1") {
                    const bz = new Uint8Array(64);
                    buffOutLEM.set(bz, 64*((2 ** power) - 1 ));
                }
                */

                const promiseWrite = fdNew.write(buffOutLEM);
                const buffOutC = await G.batchLEMtoC(buffOutLEM);

                responseHasher.update(buffOutC);
                await promiseWrite;
                if (i==0)   // Return the 2 first points.
                    for (let j=0; j<Math.min(2, NPoints); j++)
                        res.push(G.fromRprLEM(buffOutLEM, j*sG));
                t = curve.Fr.mul(t, curve.Fr.exp(inc, n));
            }

            await endWriteSection(fdNew);

            return res;
        }


        async function hashSection(fdTo, groupName, sectionId, nPoints, sectionName, logger) {

            const G = curve[groupName];
            const sG = G.F.n8*2;
            const nPointsChunk = Math.floor((1<<24)/sG);

            const oldPos = fdTo.pos;
            fdTo.pos = startSections[sectionId];

            for (let i=0; i< nPoints; i += nPointsChunk) {
                if (logger) logger.debug(`Hashing ${sectionName}: ${i}/${nPoints}`);
                const n = Math.min(nPoints-i, nPointsChunk);

                const buffLEM = await fdTo.read(n * sG);

                const buffU = await G.batchLEMtoU(buffLEM);

                nextChallengeHasher.update(buffU);
            }

            fdTo.pos = oldPos;
        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function contribute(oldPtauFilename, newPTauFilename, name, entropy, logger) {
        await blake2bWasm.ready();

        const {fd: fdOld, sections} = await readBinFile(oldPtauFilename, "ptau", 1);
        const {curve, power, ceremonyPower} = await readPTauHeader(fdOld, sections);
        if (power != ceremonyPower) {
            if (logger) logger.error("This file has been reduced. You cannot contribute into a reduced file.");
            throw new Error("This file has been reduced. You cannot contribute into a reduced file.");
        }
        if (sections[12]) {
            if (logger) logger.warn("WARNING: Contributing into a file that has phase2 calculated. You will have to prepare phase2 again.");
        }
        const contributions = await readContributions(fdOld, curve, sections);
        const curContribution = {
            name: name,
            type: 0, // Beacon
        };

        let lastChallengeHash;

        const rng = await getRandomRng(entropy);

        if (contributions.length>0) {
            lastChallengeHash = contributions[contributions.length-1].nextChallenge;
        } else {
            lastChallengeHash = calculateFirstChallengeHash(curve, power, logger);
        }

        // Generate a random key


        curContribution.key = createPTauKey(curve, lastChallengeHash, rng);


        const responseHasher = new blake2bWasm(64);
        responseHasher.update(lastChallengeHash);

        const fdNew = await createBinFile(newPTauFilename, "ptau", 1, 7);
        await writePTauHeader(fdNew, curve, power);

        const startSections = [];

        let firstPoints;
        firstPoints = await processSection(2, "G1",  (2 ** power) * 2 -1, curve.Fr.e(1), curContribution.key.tau.prvKey, "tauG1" );
        curContribution.tauG1 = firstPoints[1];
        firstPoints = await processSection(3, "G2",  (2 ** power) , curve.Fr.e(1), curContribution.key.tau.prvKey, "tauG2" );
        curContribution.tauG2 = firstPoints[1];
        firstPoints = await processSection(4, "G1",  (2 ** power) , curContribution.key.alpha.prvKey, curContribution.key.tau.prvKey, "alphaTauG1" );
        curContribution.alphaG1 = firstPoints[0];
        firstPoints = await processSection(5, "G1",  (2 ** power) , curContribution.key.beta.prvKey, curContribution.key.tau.prvKey, "betaTauG1" );
        curContribution.betaG1 = firstPoints[0];
        firstPoints = await processSection(6, "G2",  1, curContribution.key.beta.prvKey, curContribution.key.tau.prvKey, "betaTauG2" );
        curContribution.betaG2 = firstPoints[0];

        curContribution.partialHash = responseHasher.getPartialHash();

        const buffKey = new Uint8Array(curve.F1.n8*2*6+curve.F2.n8*2*3);

        toPtauPubKeyRpr(buffKey, 0, curve, curContribution.key, false);

        responseHasher.update(new Uint8Array(buffKey));
        const hashResponse = responseHasher.digest();

        if (logger) logger.info(formatHash(hashResponse, "Contribution Response Hash imported: "));

        const nextChallengeHasher = new blake2bWasm(64);
        nextChallengeHasher.update(hashResponse);

        await hashSection(fdNew, "G1", 2, (2 ** power) * 2 -1, "tauG1");
        await hashSection(fdNew, "G2", 3, (2 ** power)       , "tauG2");
        await hashSection(fdNew, "G1", 4, (2 ** power)       , "alphaTauG1");
        await hashSection(fdNew, "G1", 5, (2 ** power)       , "betaTauG1");
        await hashSection(fdNew, "G2", 6, 1                  , "betaG2");

        curContribution.nextChallenge = nextChallengeHasher.digest();

        if (logger) logger.info(formatHash(curContribution.nextChallenge, "Next Challenge Hash: "));

        contributions.push(curContribution);

        await writeContributions(fdNew, curve, contributions);

        await fdOld.close();
        await fdNew.close();

        return hashResponse;

        async function processSection(sectionId, groupName, NPoints, first, inc, sectionName) {
            const res = [];
            fdOld.pos = sections[sectionId][0].p;

            await startWriteSection(fdNew, sectionId);

            startSections[sectionId] = fdNew.pos;

            const G = curve[groupName];
            const sG = G.F.n8*2;
            const chunkSize = Math.floor((1<<20) / sG);   // 128Mb chunks
            let t = first;
            for (let i=0 ; i<NPoints ; i+= chunkSize) {
                if (logger) logger.debug(`processing: ${sectionName}: ${i}/${NPoints}`);
                const n= Math.min(NPoints-i, chunkSize );
                const buffIn = await fdOld.read(n * sG);
                const buffOutLEM = await G.batchApplyKey(buffIn, t, inc);

                /* Code to test the case where we don't have the 2^m-2 component
                if (sectionName== "tauG1") {
                    const bz = new Uint8Array(64);
                    buffOutLEM.set(bz, 64*((2 ** power) - 1 ));
                }
                */

                const promiseWrite = fdNew.write(buffOutLEM);
                const buffOutC = await G.batchLEMtoC(buffOutLEM);

                responseHasher.update(buffOutC);
                await promiseWrite;
                if (i==0)   // Return the 2 first points.
                    for (let j=0; j<Math.min(2, NPoints); j++)
                        res.push(G.fromRprLEM(buffOutLEM, j*sG));
                t = curve.Fr.mul(t, curve.Fr.exp(inc, n));
            }

            await endWriteSection(fdNew);

            return res;
        }


        async function hashSection(fdTo, groupName, sectionId, nPoints, sectionName) {

            const G = curve[groupName];
            const sG = G.F.n8*2;
            const nPointsChunk = Math.floor((1<<24)/sG);

            const oldPos = fdTo.pos;
            fdTo.pos = startSections[sectionId];

            for (let i=0; i< nPoints; i += nPointsChunk) {
                if ((logger)&&i) logger.debug(`Hashing ${sectionName}: ` + i);
                const n = Math.min(nPoints-i, nPointsChunk);

                const buffLEM = await fdTo.read(n * sG);

                const buffU = await G.batchLEMtoU(buffLEM);

                nextChallengeHasher.update(buffU);
            }

            fdTo.pos = oldPos;
        }


    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function preparePhase2(oldPtauFilename, newPTauFilename, logger) {

        const {fd: fdOld, sections} = await readBinFile(oldPtauFilename, "ptau", 1);
        const {curve, power} = await readPTauHeader(fdOld, sections);

        const fdNew = await createBinFile(newPTauFilename, "ptau", 1, 11);
        await writePTauHeader(fdNew, curve, power);

        await copySection(fdOld, sections, fdNew, 2);
        await copySection(fdOld, sections, fdNew, 3);
        await copySection(fdOld, sections, fdNew, 4);
        await copySection(fdOld, sections, fdNew, 5);
        await copySection(fdOld, sections, fdNew, 6);
        await copySection(fdOld, sections, fdNew, 7);

        await processSection(2, 12, "G1", "tauG1" );
        await processSection(3, 13, "G2", "tauG2" );
        await processSection(4, 14, "G1", "alphaTauG1" );
        await processSection(5, 15, "G1", "betaTauG1" );

        await fdOld.close();
        await fdNew.close();

        // await fs.promises.unlink(newPTauFilename+ ".tmp");

        return;

        async function processSection(oldSectionId, newSectionId, Gstr, sectionName) {
            if (logger) logger.debug("Starting section: "+sectionName);

            await startWriteSection(fdNew, newSectionId);

            for (let p=0; p<=power; p++) {
                await processSectionPower(p);
            }

            if (oldSectionId == 2) {
                await processSectionPower(power+1);
            }

            await endWriteSection(fdNew);


            async function processSectionPower(p) {
                const nPoints = 2 ** p;
                const G = curve[Gstr];
                curve.Fr;
                const sGin = G.F.n8*2;
                G.F.n8*3;

                let buff;
                buff = new BigBuffer(nPoints*sGin);

                await startReadUniqueSection(fdOld, sections, oldSectionId);
                if ((oldSectionId == 2)&&(p==power+1)) {
                    await fdOld.readToBuffer(buff, 0,(nPoints-1)*sGin );
                    buff.set(curve.G1.zeroAffine, (nPoints-1)*sGin );
                } else {
                    await fdOld.readToBuffer(buff, 0,nPoints*sGin );
                }
                await endReadSection(fdOld, true);


                buff = await G.lagrangeEvaluations(buff, "affine", "affine", logger, sectionName);
                await fdNew.write(buff);

    /*
                if (p <= curve.Fr.s) {
                    buff = await G.ifft(buff, "affine", "affine", logger, sectionName);
                    await fdNew.write(buff);
                } else if (p == curve.Fr.s+1) {
                    const smallM = 1<<curve.Fr.s;
                    let t0 = new BigBuffer( smallM * sGmid );
                    let t1 = new BigBuffer( smallM * sGmid );

                    const shift_to_small_m = Fr.exp(Fr.shift, smallM);
                    const one_over_denom = Fr.inv(Fr.sub(shift_to_small_m, Fr.one));

                    let sInvAcc = Fr.one;
                    for (let i=0; i<smallM; i++) {
                        const ti =  buff.slice(i*sGin, (i+1)*sGin);
                        const tmi = buff.slice((i+smallM)*sGin, (i+smallM+1)*sGin);

                        t0.set(
                            G.timesFr(
                                G.sub(
                                    G.timesFr(ti , shift_to_small_m),
                                    tmi
                                ),
                                one_over_denom
                            ),
                            i*sGmid
                        );
                        t1.set(
                            G.timesFr(
                                G.sub( tmi, ti),
                                Fr.mul(sInvAcc, one_over_denom)
                            ),
                            i*sGmid
                        );


                        sInvAcc = Fr.mul(sInvAcc, Fr.shiftInv);
                    }
                    t0 = await G.ifft(t0, "jacobian", "affine", logger, sectionName + " t0");
                    await fdNew.write(t0);
                    t0 = null;
                    t1 = await G.ifft(t1, "jacobian", "affine", logger, sectionName + " t0");
                    await fdNew.write(t1);

                } else {
                    if (logger) logger.error("Power too big");
                    throw new Error("Power to big");
                }
    */
            }
        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function truncate(ptauFilename, template, logger) {

        const {fd: fdOld, sections} = await readBinFile(ptauFilename, "ptau", 1);
        const {curve, power, ceremonyPower} = await readPTauHeader(fdOld, sections);

        const sG1 = curve.G1.F.n8*2;
        const sG2 = curve.G2.F.n8*2;

        for (let p=1; p<power; p++) {
            await generateTruncate(p);
        }

        await fdOld.close();

        return true;

        async function generateTruncate(p) {

            let sP = p.toString();
            while (sP.length<2) sP = "0" + sP;

            if (logger) logger.debug("Writing Power: "+sP);

            const fdNew = await createBinFile(template + sP + ".ptau", "ptau", 1, 11);
            await writePTauHeader(fdNew, curve, p, ceremonyPower);

            await copySection(fdOld, sections, fdNew, 2, ((2 ** p)*2-1) * sG1 ); // tagG1
            await copySection(fdOld, sections, fdNew, 3, (2 ** p) * sG2); // tauG2
            await copySection(fdOld, sections, fdNew, 4, (2 ** p) * sG1); // alfaTauG1
            await copySection(fdOld, sections, fdNew, 5, (2 ** p) * sG1); // betaTauG1
            await copySection(fdOld, sections, fdNew, 6,  sG2); // betaTauG2
            await copySection(fdOld, sections, fdNew, 7); // contributions
            await copySection(fdOld, sections, fdNew, 12, ((2 ** (p+1))*2 -1) * sG1); // L_tauG1
            await copySection(fdOld, sections, fdNew, 13, ((2 ** p)*2 -1) * sG2); // L_tauG2
            await copySection(fdOld, sections, fdNew, 14, ((2 ** p)*2 -1) * sG1); // L_alfaTauG1
            await copySection(fdOld, sections, fdNew, 15, ((2 ** p)*2 -1) * sG1); // L_betaTauG1

            await fdNew.close();
        }


    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function convert(oldPtauFilename, newPTauFilename, logger) {

        const {fd: fdOld, sections} = await readBinFile(oldPtauFilename, "ptau", 1);
        const {curve, power} = await readPTauHeader(fdOld, sections);

        const fdNew = await createBinFile(newPTauFilename, "ptau", 1, 11);
        await writePTauHeader(fdNew, curve, power);

        // const fdTmp = await fastFile.createOverride(newPTauFilename+ ".tmp");

        await copySection(fdOld, sections, fdNew, 2);
        await copySection(fdOld, sections, fdNew, 3);
        await copySection(fdOld, sections, fdNew, 4);
        await copySection(fdOld, sections, fdNew, 5);
        await copySection(fdOld, sections, fdNew, 6);
        await copySection(fdOld, sections, fdNew, 7);

        await processSection(2, 12, "G1", "tauG1" );
        await copySection(fdOld, sections, fdNew, 13);
        await copySection(fdOld, sections, fdNew, 14);
        await copySection(fdOld, sections, fdNew, 15);

        await fdOld.close();
        await fdNew.close();

        // await fs.promises.unlink(newPTauFilename+ ".tmp");

        return;

        async function processSection(oldSectionId, newSectionId, Gstr, sectionName) {
            if (logger) logger.debug("Starting section: "+sectionName);

            await startWriteSection(fdNew, newSectionId);

            const size = sections[newSectionId][0].size;
            const chunkSize = fdOld.pageSize;
            await startReadUniqueSection(fdOld, sections, newSectionId);
            for (let p=0; p<size; p+=chunkSize) {
                const l = Math.min(size -p, chunkSize);
                const buff = await fdOld.read(l);
                await fdNew.write(buff);
            }
            await endReadSection(fdOld);

            if (oldSectionId == 2) {
                await processSectionPower(power+1);
            }

            await endWriteSection(fdNew);

            async function processSectionPower(p) {
                const nPoints = 2 ** p;
                const G = curve[Gstr];
                const sGin = G.F.n8*2;

                let buff;
                buff = new BigBuffer(nPoints*sGin);

                await startReadUniqueSection(fdOld, sections, oldSectionId);
                if ((oldSectionId == 2)&&(p==power+1)) {
                    await fdOld.readToBuffer(buff, 0,(nPoints-1)*sGin );
                    buff.set(curve.G1.zeroAffine, (nPoints-1)*sGin );
                } else {
                    await fdOld.readToBuffer(buff, 0,nPoints*sGin );
                }
                await endReadSection(fdOld, true);

                buff = await G.lagrangeEvaluations(buff, "affine", "affine", logger, sectionName);
                await fdNew.write(buff);

    /*
                if (p <= curve.Fr.s) {
                    buff = await G.ifft(buff, "affine", "affine", logger, sectionName);
                    await fdNew.write(buff);
                } else if (p == curve.Fr.s+1) {
                    const smallM = 1<<curve.Fr.s;
                    let t0 = new BigBuffer( smallM * sGmid );
                    let t1 = new BigBuffer( smallM * sGmid );

                    const shift_to_small_m = Fr.exp(Fr.shift, smallM);
                    const one_over_denom = Fr.inv(Fr.sub(shift_to_small_m, Fr.one));

                    let sInvAcc = Fr.one;
                    for (let i=0; i<smallM; i++) {
                        if (i%10000) logger.debug(`sectionName prepare L calc: ${sectionName}, ${i}/${smallM}`);
                        const ti =  buff.slice(i*sGin, (i+1)*sGin);
                        const tmi = buff.slice((i+smallM)*sGin, (i+smallM+1)*sGin);

                        t0.set(
                            G.timesFr(
                                G.sub(
                                    G.timesFr(ti , shift_to_small_m),
                                    tmi
                                ),
                                one_over_denom
                            ),
                            i*sGmid
                        );
                        t1.set(
                            G.timesFr(
                                G.sub( tmi, ti),
                                Fr.mul(sInvAcc, one_over_denom)
                            ),
                            i*sGmid
                        );


                        sInvAcc = Fr.mul(sInvAcc, Fr.shiftInv);
                    }
                    t0 = await G.ifft(t0, "jacobian", "affine", logger, sectionName + " t0");
                    await fdNew.write(t0);
                    t0 = null;
                    t1 = await G.ifft(t1, "jacobian", "affine", logger, sectionName + " t1");
                    await fdNew.write(t1);

                } else {
                    if (logger) logger.error("Power too big");
                    throw new Error("Power to big");
                }
    */
            }


        }
    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    async function exportJson(pTauFilename, verbose) {
        const {fd, sections} = await readBinFile(pTauFilename, "ptau", 1);

        const {curve, power} = await readPTauHeader(fd, sections);

        const pTau = {};
        pTau.q = curve.q;
        pTau.power = power;
        pTau.contributions = await readContributions(fd, curve, sections);

        pTau.tauG1 = await exportSection(2, "G1", (2 ** power)*2 -1, "tauG1");
        pTau.tauG2 = await exportSection(3, "G2", (2 ** power), "tauG2");
        pTau.alphaTauG1 = await exportSection(4, "G1", (2 ** power), "alphaTauG1");
        pTau.betaTauG1 = await exportSection(5, "G1", (2 ** power), "betaTauG1");
        pTau.betaG2 = await exportSection(6, "G2", 1, "betaG2");

        pTau.lTauG1 = await exportLagrange(12, "G1", "lTauG1");
        pTau.lTauG2 = await exportLagrange(13, "G2", "lTauG2");
        pTau.lAlphaTauG1 = await exportLagrange(14, "G1", "lAlphaTauG2");
        pTau.lBetaTauG1 = await exportLagrange(15, "G1", "lBetaTauG2");

        await fd.close();

        return pTau;



        async function exportSection(sectionId, groupName, nPoints, sectionName) {
            const G = curve[groupName];
            const sG = G.F.n8*2;

            const res = [];
            await startReadUniqueSection(fd, sections, sectionId);
            for (let i=0; i< nPoints; i++) {
                if ((verbose)&&i&&(i%10000 == 0)) console.log(`${sectionName}: ` + i);
                const buff = await fd.read(sG);
                res.push(G.fromRprLEM(buff, 0));
            }
            await endReadSection(fd);

            return res;
        }

        async function exportLagrange(sectionId, groupName, sectionName) {
            const G = curve[groupName];
            const sG = G.F.n8*2;

            const res = [];
            await startReadUniqueSection(fd, sections, sectionId);
            for (let p=0; p<=power; p++) {
                if (verbose) console.log(`${sectionName}: Power: ${p}`);
                res[p] = [];
                const nPoints = (2 ** p);
                for (let i=0; i<nPoints; i++) {
                    if ((verbose)&&i&&(i%10000 == 0)) console.log(`${sectionName}: ${i}/${nPoints}`);
                    const buff = await fd.read(sG);
                    res[p].push(G.fromRprLEM(buff, 0));
                }
            }
            await endReadSection(fd);
            return res;
        }


    }

    /*
        Copyright 2018 0KIMS association.

        This file is part of snarkJS.

        snarkJS is a free software: you can redistribute it and/or modify it
        under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        snarkJS is distributed in the hope that it will be useful, but WITHOUT
        ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
        or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
        License for more details.

        You should have received a copy of the GNU General Public License
        along with snarkJS. If not, see <https://www.gnu.org/licenses/>.
    */

    var powersoftau = /*#__PURE__*/Object.freeze({
